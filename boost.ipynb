{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn\n",
    "\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "if device == torch.device(\"cuda:0\"):\n",
    "  print('Everything looks good; continue')\n",
    "else:\n",
    "  print('GPU is not detected. Make sure you have chosen the right runtime type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "img_path_df = df['id'].apply(lambda x: os.path.join('./data/train_images', f'{x}.jpeg'))\n",
    "img_path_test_df = test_df['id'].apply(lambda x: os.path.join('./data/test_images', f'{x}.jpeg'))\n",
    "\n",
    "\n",
    "df.insert(1, 'img_path', img_path_df)\n",
    "test_df.insert(1, 'img_path', img_path_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "  # X4_mean,X11_mean,X18_mean,X26_mean,X50_mean,X3112_mean\n",
    "  TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']\n",
    "  PRED_COLUMNS = ['X4', 'X11', 'X18', 'X26', 'X50', 'X3112']\n",
    "  # remove target columns and id\n",
    "  FEATURE_COLUMNS = df.columns.drop(['id', 'X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean', 'img_path']).to_list()\n",
    "\n",
    "  BATCH_SIZE = 64\n",
    "  NUM_EPOCHS = 6\n",
    "\n",
    "  NUM_TARGETS = 6\n",
    "  NUM_FEATURES = 163\n",
    "\n",
    "  LEARNING_RATE = 0.001\n",
    "  WEIGHT_DECAY = 0.0001\n",
    "\n",
    "  SEED = 42\n",
    "\n",
    "print(CFG.TARGET_COLUMNS)\n",
    "print(CFG.PRED_COLUMNS)\n",
    "print(CFG.FEATURE_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 98% upper_quantile and 0.1% lower_quantile\n",
    "print(df.shape)\n",
    "for col in CFG.TARGET_COLUMNS:\n",
    "  upper_quantile = df[col].quantile(0.98)\n",
    "  lower_quantile = df[col].quantile(0.001)\n",
    "  df = df[df[col] < upper_quantile ]\n",
    "  df = df[df[col] > lower_quantile ]\n",
    "print(df.shape)\n",
    "df[CFG.TARGET_COLUMNS].hist(bins=50, figsize=(20, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=CFG.SEED)\n",
    "print(train_df.shape, val_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, df, transform):\n",
    "  model.eval()\n",
    "  embeddings = []\n",
    "  with torch.no_grad():\n",
    "    for i in tqdm(range(0,len(df), CFG.BATCH_SIZE)):\n",
    "      img_paths = df['img_path'][i: i+CFG.BATCH_SIZE].values\n",
    "      images = torch.stack([transform(read_image(img_path)) for img_path in img_paths])\n",
    "      embedding = model(images.to(device))\n",
    "      embeddings.append(embedding.cpu())\n",
    "  return torch.cat(embeddings)\n",
    "\n",
    "\n",
    "train_transform =v2.Compose([\n",
    "  v2.Resize(140),\n",
    "  v2.ToImage(), \n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "  v2.Resize(140),\n",
    "  v2.ToImage(), \n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "  v2.Resize(140),\n",
    "  v2.ToImage(), \n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg').to(device)\n",
    "\n",
    "train_embeddings = get_embeddings(model, train_df, train_transform)\n",
    "val_embeddings = get_embeddings(model, val_df,  val_transform)\n",
    "test_embeddings = get_embeddings(model, test_df, test_transform)\n",
    "\n",
    "print(\"Train embeddings: \",train_embeddings.shape)\n",
    "print(\"Val embeddings: \",val_embeddings.shape)\n",
    "print(\"Test embeddings: \",test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_embedding(df, embeddings):\n",
    "  return pd.concat([df, pd.DataFrame(embeddings)], axis=1)\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_final_df= cat_embedding(df, train_embeddings)\n",
    "val_final_df = cat_embedding(val_df, val_embeddings)\n",
    "test_final_df = cat_embedding(test_df, test_embeddings)\n",
    "\n",
    "print(\"Train final: \", train_final_df.shape)\n",
    "print(\"Val final: \", val_final_df.shape)\n",
    "print(\"Test final: \", test_final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final data(optional)\n",
    "train_final_df.to_csv('./data/train_final.csv', index=False)  \n",
    "val_final_df.to_csv('./data/val_final.csv', index=False)\n",
    "test_final_df.to_csv('./data/test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the final data\n",
    "# train_final_df = pd.read_csv('./data/train_final.csv')\n",
    "# val_final_df = pd.read_csv('./data/val_final.csv')\n",
    "# test_final_df = pd.read_csv('./data/test_final.csv')\n",
    "\n",
    "X_train = train_final_df.drop(['id', 'img_path']+CFG.TARGET_COLUMNS, axis=1)\n",
    "y_train = train_final_df[CFG.TARGET_COLUMNS]\n",
    "X_val = val_final_df.drop(['id', 'img_path']+CFG.TARGET_COLUMNS, axis=1)\n",
    "y_val = val_final_df[CFG.TARGET_COLUMNS]\n",
    "params = {\n",
    "    'learning_rate': 0.005,\n",
    "\n",
    "    \"num_iterations\": 10000,\n",
    "    \"bagging_freq\": 7,\n",
    "    \"bagging_fraction\": 0.75,\n",
    "    \"feature_fraction\": 0.75,\n",
    "    'lambda_l1': 0.01,  # Reduce regularization\n",
    "    'lambda_l2': 0.01,\n",
    "}\n",
    "\n",
    "print(\"X_train.values: \", X_train.values.shape)\n",
    "\n",
    "lgbm_model = lgb.LGBMRegressor(**params)\n",
    "sklearn_model = MultiOutputRegressor(lgbm_model)\n",
    "sklearn_model.fit(\n",
    "  X = X_train,\n",
    "  y = y_train,\n",
    ")\n",
    "\n",
    "\n",
    "score = sklearn_model.score(X_val, y_val)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "# test\n",
    "X_test = test_final_df.drop(['id', 'img_path'], axis=1)\n",
    "y_test = sklearn_model.predict(X_test)\n",
    "\n",
    "#concatenate test_df[ids] to the predictions\n",
    "ids = test_df.iloc[:,0].values\n",
    "y_test = np.concatenate((ids.reshape(-1,1), y_test), axis=1)\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "  y_test,\n",
    "  columns=[\"id\"] + CFG.PRED_COLUMNS\n",
    ")\n",
    "preds_df[\"id\"] = preds_df[\"id\"].astype(int)\n",
    "  \n",
    "preds_df[CFG.PRED_COLUMNS].hist(bins=100, figsize=(20, 10))\n",
    "preds_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train_final_df.drop(['id', 'img_path']+CFG.TARGET_COLUMNS, axis=1)\n",
    "y_train = train_final_df[CFG.TARGET_COLUMNS]\n",
    "X_val = val_final_df.drop(['id', 'img_path']+CFG.TARGET_COLUMNS, axis=1)\n",
    "y_val = val_final_df[CFG.TARGET_COLUMNS]\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    learning_rate=0.005,\n",
    "    random_state=CFG.SEED,\n",
    "    multi_strategy='multi_output_tree',\n",
    "    eval_metric=sklearn.metrics.mean_squared_error,\n",
    "    n_estimators=1000,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "sklearn_model = MultiOutputRegressor(xgb_model)\n",
    "sklearn_model.fit(\n",
    "  X = X_train,\n",
    "  y = y_train,\n",
    ")\n",
    "     \n",
    "\n",
    "\n",
    "score = sklearn_model.score(X_val, y_val)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "# test\n",
    "X_test = test_final_df.drop(['id', 'img_path'], axis=1)\n",
    "y_test = sklearn_model.predict(X_test)\n",
    "\n",
    "#concatenate test_df[ids] to the predictions\n",
    "ids = test_df.iloc[:,0].values\n",
    "y_test = np.concatenate((ids.reshape(-1,1), y_test), axis=1)\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "  y_test,\n",
    "  columns=[\"id\"] + CFG.PRED_COLUMNS\n",
    ")\n",
    "preds_df[\"id\"] = preds_df[\"id\"].astype(int)\n",
    "  \n",
    "preds_df[CFG.PRED_COLUMNS].hist(bins=100, figsize=(20, 10))\n",
    "preds_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "X_train = train_final_df.drop(['id', 'img_path']+CFG.TARGET_COLUMNS, axis=1)\n",
    "y_train = train_final_df[CFG.TARGET_COLUMNS]\n",
    "X_val = val_final_df.drop(['id', 'img_path']+CFG.TARGET_COLUMNS, axis=1)\n",
    "y_val = val_final_df[CFG.TARGET_COLUMNS]\n",
    "\n",
    "params = {'learning_rate': 0.05, \n",
    "          'loss_function': 'MultiRMSE', \n",
    "          'eval_metric': 'MultiRMSE', \n",
    "          'task_type': 'GPU', \n",
    "          'iterations': 10000,\n",
    "          'boosting_type': 'Plain', \n",
    "         }\n",
    "\n",
    "cat_model = CatBoostRegressor(**params) \n",
    "\n",
    "cat_model.fit(\n",
    "  X = X_train,\n",
    "  y = y_train,\n",
    "  eval_set=(X_val, y_val),\n",
    ")\n",
    "\n",
    "score = cat_model.score(X_val, y_val)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "\n",
    "# test\n",
    "X_test = test_final_df.drop(['id', 'img_path'], axis=1)\n",
    "y_test = cat_model.predict(X_test)\n",
    "\n",
    "#concatenate test_df[ids] to the predictions\n",
    "ids = test_df.iloc[:,0].values\n",
    "y_test = np.concatenate((ids.reshape(-1,1), y_test), axis=1)\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "  y_test,\n",
    "  columns=[\"id\"] + CFG.PRED_COLUMNS\n",
    ")\n",
    "preds_df[\"id\"] = preds_df[\"id\"].astype(int)\n",
    "  \n",
    "preds_df[CFG.PRED_COLUMNS].hist(bins=100, figsize=(20, 10))\n",
    "preds_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "X_test = test_final_df.drop(['id', 'img_path'], axis=1)\n",
    "\n",
    "y_test = sklearn_model.predict(X_test)\n",
    "# y_test = cat_model.predict(X_test)\n",
    "\n",
    "#concatenate test_df[ids] to the predictions\n",
    "ids = test_df.iloc[:,0].values\n",
    "y_test = np.concatenate((ids.reshape(-1,1), y_test), axis=1)\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "  y_test,\n",
    "  columns=[\"id\"] + CFG.PRED_COLUMNS\n",
    ")\n",
    "preds_df[\"id\"] = preds_df[\"id\"].astype(int)\n",
    "\n",
    "\n",
    "# denormalize the predictions\n",
    "# preds_df[CFG.TARGET_COLUMNS] = target_scaler.inverse_transform(preds_df[CFG.TARGET_COLUMNS])\n",
    "# preds_df[CFG.PRED_COLUMNS] = np.expm1(preds_df[CFG.PRED_COLUMNS])\n",
    "  \n",
    "preds_df[CFG.PRED_COLUMNS].hist(bins=100, figsize=(20, 10))\n",
    "\n",
    "preds_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs680",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
